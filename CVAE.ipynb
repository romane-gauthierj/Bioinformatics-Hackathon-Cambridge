{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset, random_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import t, shapiro, kstest\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data preparation\n",
    "def prepare_data():\n",
    "    #loading data\n",
    "    abundance = pd.read_csv('~/icr/simko/data/simko2_data/passport_prots.csv', index_col=0)\n",
    "    abundance.index = abundance.index.astype(str)\n",
    "    #removing cell lines with over 4000 nans\n",
    "    nans_per_cl = abundance.isna().sum(axis=0)\n",
    "    abundance_cl_filtered = abundance.loc[:, nans_per_cl<4000]\n",
    "    #getting rid of protein with over 80% NaN (from the dataset filtered by CLs)\n",
    "    prot_nan_count = abundance_cl_filtered.isna().sum(axis=1)\n",
    "    prot_nan_percent = (prot_nan_count/abundance_cl_filtered.shape[1])*100\n",
    "    abundance_filtered = abundance_cl_filtered[prot_nan_percent<80]\n",
    "\n",
    "    #imputing witht the lower quartile average for each protein\n",
    "    #set the protein names as the index - ignores it while we find the lower quartile\n",
    "    def average_lower_quartile(x):\n",
    "        sorted_abundances = x.dropna().sort_values()\n",
    "        lower_qt_values = sorted_abundances.iloc[:int(len(sorted_abundances) * 0.25)]\n",
    "        return lower_qt_values.mean()\n",
    "\n",
    "\n",
    "    lower_qt_averages = abundance_filtered.apply(average_lower_quartile, axis=1)\n",
    "    abundance_filtered_no_nan = abundance_filtered.apply(lambda x: x.fillna(lower_qt_averages[x.name]), axis=1)\n",
    "\n",
    "    #transposing\n",
    "    abundance_imputed = abundance_filtered_no_nan.T\n",
    "    #scaling the imputed data\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = pd.DataFrame(scaler.fit_transform(abundance_imputed), index=abundance_imputed.index, columns=abundance_imputed.columns)\n",
    "    #scaled_data = scaled_data.T\n",
    "    return scaled_data\n",
    "\n",
    "scaled_data = prepare_data()\n",
    "scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting pbrm1 values so we can make the continuous - put the values between 0 and 1\n",
    "raw_pbrm1 = scaled_data[\"PBRM1\"].values\n",
    "raw_min, raw_max = raw_pbrm1.min(), raw_pbrm1.max()\n",
    "#condition = \n",
    "c = (raw_pbrm1 - raw_min) / (raw_max - raw_min)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting rid of pbrm1 from data set to use in training\n",
    "X = scaled_data.drop(columns=[\"PBRM1\"]).values.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating custom data set --> makes it easier to proccess it later on (data augmentation)\n",
    "class ProteomeDataset(Dataset):\n",
    "    def __init__(self, X, c):\n",
    "        self.X = torch.from_numpy(X).float()\n",
    "        self.c = torch.from_numpy(c).float().unsqueeze(-1)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.c[i]\n",
    "\n",
    "full_ds = ProteomeDataset(X, c)\n",
    "loader  = DataLoader(full_ds, batch_size=32, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting into train/validation set to check model isnt over fitting \n",
    "# 20% validation\n",
    "n_val = int(len(full_ds) * 0.2)\n",
    "n_train = len(full_ds) - n_val\n",
    "train_ds, val_ds = random_split(full_ds, [n_train, n_val])\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=32, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(nn.Module):\n",
    "    def __init__(self, n_proteins, latent_dim=50):\n",
    "        super().__init__()\n",
    "        self.enc = nn.Sequential(\n",
    "            nn.Linear(n_proteins + 1, 1024),\n",
    "            nn.BatchNorm1d(1024), nn.ReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512), nn.ReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "\n",
    "            nn.Linear(512, 128),\n",
    "            nn.BatchNorm1d(128), nn.ReLU(),\n",
    "            nn.Dropout(p=0.3)\n",
    "\n",
    "        )\n",
    "        self.fc_mu     = nn.Linear(128, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(128, latent_dim)\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.Linear(latent_dim + 1, 128),\n",
    "            nn.BatchNorm1d(128), nn.ReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "\n",
    "            nn.Linear(128, 512),\n",
    "            nn.BatchNorm1d(512), nn.ReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.BatchNorm1d(1024), nn.ReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "\n",
    "            nn.Linear(1024, n_proteins)\n",
    "            #nn.BatchNorm1d(4096), nn.ReLU(), \n",
    "            #nn.Dropout(p=0.3),\n",
    "\n",
    "            #nn.Linear(4096, n_proteins)\n",
    "        )\n",
    "\n",
    "    def encode(self, x, c):\n",
    "        h = self.enc(torch.cat([x, c], dim=1))\n",
    "        mu, logvar = self.fc_mu(h), self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = (0.5 * logvar).exp()\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        mu, logvar = self.encode(x, c)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_rec = self.dec(torch.cat([z, c], dim=1))\n",
    "        return x_rec, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss function - we will do KL annealing in the training loop\n",
    "def loss_function(recon_x, x_batch, mu, logvar, beta=1.0):\n",
    "    \"\"\"\n",
    "    recon_x:   (B, P)  reconstructed batch\n",
    "    x:         (B, P)  original batch\n",
    "    mu:        (B, L)  latent means\n",
    "    logvar:    (B, L)  latent log‐variances\n",
    "    beta:      float   weight on the KL term\n",
    "    \"\"\"\n",
    "    # Reconstruction loss (sum over batch & features)\n",
    "    batch_size = x_batch.size(0)\n",
    "    recon_loss = nn.MSELoss(reduction='sum')(recon_x, x_batch)\n",
    "    # KL divergence term (sum over batch & latent dims)\n",
    "    kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "    #normlaising loss by batch size\n",
    "    total_l = (recon_loss + beta * kld) / batch_size\n",
    "    return total_l, recon_loss / batch_size, kld / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying huber loss instead\n",
    "def loss_function_huber(recon_x, x_batch, mu, logvar, beta=1.0):\n",
    "    # Reconstruction loss (sum over batch & features)\n",
    "    batch_size = x_batch.size(0)\n",
    "    recon_loss = nn.SmoothL1Loss(reduction='sum')(recon_x, x_batch)\n",
    "    # KL divergence term (sum over batch & latent dims)\n",
    "    kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "    #normlaising loss by batch size\n",
    "    total_l = (recon_loss + beta * kld) / batch_size\n",
    "    return total_l, recon_loss / batch_size, kld / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up model and optimiser\n",
    "#dont need to include conditional dimension the model architecture already knows to expect the 'condtion vector' --> the plus 1 is the extra 'value' that is the pbrm1 condition\n",
    "model  = CVAE(X.shape[1], latent_dim=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "#KL annealing\n",
    "def beta_schedule(epoch, warmup=50):\n",
    "    return min(1.0, epoch / warmup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_losses = []\n",
    "recon_losses = []\n",
    "kl_losses = []\n",
    "\n",
    "val_total_losses = []\n",
    "val_recon_losses = []\n",
    "val_kl_losses = []\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    total_loss = 0\n",
    "    recon = 0\n",
    "    KL = 0\n",
    "    β = beta_schedule(epoch)     # e.g. linear ramp 0→1 over first 50 epochs\n",
    "    #β = 1\n",
    "    for x_batch, c_batch in train_loader:\n",
    "        x_batch, c_batch = x_batch, c_batch\n",
    "\n",
    "        # jitter c if you’re using augmentation\n",
    "        noise   = torch.randn_like(c_batch) * 0.02\n",
    "        c_noisy = torch.clamp(c_batch + noise, 0.0, 1.0)\n",
    "\n",
    "        x_rec, mu, logvar = model(x_batch, c_batch)\n",
    "        loss, recon_loss, kld = loss_function_huber(x_rec, x_batch, mu, logvar, beta=β)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        recon += recon_loss.item()\n",
    "        KL += kld.item()\n",
    "        optimizer.step()\n",
    "\n",
    "    total_losses.append(total_loss / len(train_loader))\n",
    "    recon_losses.append(recon / len(train_loader))\n",
    "    kl_losses.append(KL / len(train_loader))\n",
    "        \n",
    "    # VALIDATION LOOP — added here\n",
    "    model.eval()  # Switch to eval mode for validation\n",
    "    val_total_loss = 0\n",
    "    val_recon = 0\n",
    "    val_KL = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_val, c_val in val_loader:\n",
    "            noise   = torch.randn_like(c_val) * 0.02\n",
    "            c_noisy = torch.clamp(c_val + noise, 0.0, 1.0)\n",
    "\n",
    "            x_rec, mu, logvar = model(x_val, c_val)\n",
    "            loss, recon_loss, kld = loss_function_huber(x_rec, x_val, mu, logvar, beta=β)\n",
    "\n",
    "            val_total_loss += loss.item()\n",
    "            val_recon += recon_loss.item()\n",
    "            val_KL += kld.item()\n",
    "\n",
    "    val_total_losses.append(val_total_loss / len(val_loader))\n",
    "    val_recon_losses.append(val_recon / len(val_loader))\n",
    "    val_kl_losses.append(val_KL / len(val_loader))\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    print(f\"Epoch {epoch:03d} | β={β:.2f} | Totak_Loss={total_loss/len(train_loader):.2f} | Loss={recon/len(train_loader):.2f} | Loss_per_element={recon/(896*6892):.2f} | KL={KL/len(train_loader):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting loss curves (training and validation)\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(total_losses, label=\"Total Loss\")\n",
    "plt.plot(recon_losses, label='Reconstruction Loss')\n",
    "plt.plot(val_total_losses, label=\"Total Loss (Validation)\")\n",
    "plt.plot(val_recon_losses, label=\"Reconstruction Loss (Validation)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Total & Reconstruction Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(kl_losses, label=\"KL Divergence\", color=\"orange\")\n",
    "plt.plot(val_kl_losses, label=\"KL Divergence (Validation)\", color='blue')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"KL Loss\")\n",
    "plt.title(\"KL Divergence Over Epochs\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for x_val, c_val in val_loader:\n",
    "        x_val = x_val\n",
    "\n",
    "        # --- encode with the original c_val, to capture each sample's background z ---\n",
    "        mu, logvar = model.encode(x_val, c_val)\n",
    "        z = model.reparameterize(mu, logvar)\n",
    "\n",
    "        # --- force full knockout by setting c=0 ---\n",
    "        batch_size = x_val.size(0)\n",
    "        c_knock = torch.zeros(batch_size, 1)\n",
    "\n",
    "        # --- decode at c=0 to get KO-simulated profiles ---\n",
    "        x_ko = model.dec(torch.cat([z, c_knock], dim=1))\n",
    "\n",
    "        # now x_ko has shape [batch_size, n_proteins] and is your simulated KO data\n",
    "        # you can collect these or directly compare to x_val downstream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ko = []\n",
    "all_orig = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for x_val, c_val in val_loader:\n",
    "        x_val = x_val\n",
    "        mu, logvar = model.encode(x_val, c_val)\n",
    "        z = model.reparameterize(mu, logvar)\n",
    "        c_knock = torch.zeros(x_val.size(0), 1)\n",
    "        x_ko = model.dec(torch.cat([z, c_knock], dim=1))\n",
    "        all_ko.append(x_ko.cpu())\n",
    "        all_orig.append(x_val.cpu())\n",
    "\n",
    "all_ko  = torch.cat(all_ko, dim=0).numpy()   # simulated KO\n",
    "all_orig= torch.cat(all_orig, dim=0).numpy() # original profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data_no_pbrm1 = scaled_data.drop(columns=[\"PBRM1\"])\n",
    "protein_names = scaled_data_no_pbrm1.columns.tolist()\n",
    "og_values = scaled_data_no_pbrm1.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_orig, all_ko are both shape [N_val, n_proteins]\n",
    "df_orig = pd.DataFrame(all_orig, columns=protein_names)\n",
    "df_ko   = pd.DataFrame(all_ko,   columns=protein_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_means = df_ko.mean(axis=0) - df_orig.mean(axis=0)\n",
    "# Sort by largest drop (or increase)\n",
    "delta_means = delta_means.sort_values()\n",
    "print(delta_means.head(50))   # top 10 most negative shifts\n",
    "#print(delta_means.tail(10))   # top 10 increases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Compute per-protein means\n",
    "mean_orig = df_orig.mean(axis=0)\n",
    "mean_ko   = df_ko.mean(axis=0)\n",
    "\n",
    "# 2. Build the summary DataFrame\n",
    "df_summary = pd.DataFrame({\n",
    "    'mean_orig': mean_orig,\n",
    "    'mean_ko':   mean_ko,\n",
    "})\n",
    "df_summary['diff'] = df_summary['mean_ko'] - df_summary['mean_orig']\n",
    "\n",
    "# 3. Sort by the diff column\n",
    "df_summary_sorted = df_summary.sort_values(by='diff')\n",
    "\n",
    "# 4. (Optional) Reset index so protein names become a column\n",
    "df_summary_sorted = df_summary_sorted.reset_index().rename(columns={'index':'protein'})\n",
    "\n",
    "# View the top 10 proteins most down-regulated by KO:\n",
    "df_summary_sorted.head(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t test ti see if there signigicance difference between mean in orig and mean in  ko\n",
    "from scipy.stats import ttest_ind\n",
    "p_values = {}\n",
    "for protein in df_ko.columns:\n",
    "    t_stat, p_val = ttest_ind(df_ko[protein], df_orig[protein], equal_var=False)\n",
    "    p_values[protein] = p_val\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "p_values_df = pd.DataFrame.from_dict(p_values, orient='index', columns=['p_value'])\n",
    "\n",
    "# Optionally: sort by significance\n",
    "p_values_df = p_values_df.sort_values(by='p_value')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging p values with the mean diff for each protein\n",
    "p_values_df = p_values_df.reset_index()\n",
    "p_values_df = p_values_df.rename(columns={'index': 'protein'})\n",
    "df_summary_sf = pd.merge(df_summary_sorted, p_values_df, on='protein')\n",
    "df_summary_sf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the data\n",
    "#df_summary_sorted.to_csv(\"protein_shift_summary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the df_ko and df_orig 'raw' data and the protein shift summaries along with significance\n",
    "df_summary_sf.to_csv(\"CVAE_PBRM1_ko_results(basic).csv\")\n",
    "#df_orig.to_csv(\"orig_proteindata_testsample.csv\")\n",
    "#df_ko.to_csv(\"ko_proteindata_testsample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting protein abundance changes for top 15 proteins\n",
    "import seaborn as sns\n",
    "import matplotlib.patches as mpatches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_15_prots = df_summary_sorted.head(20)\n",
    "\n",
    "#pbaf proteins\n",
    "PBAF = ('ARID2', 'PHF10', 'BRD7', 'PBRM1', 'SMARCC1', 'SMARCC2', 'SMARCE1', 'SMARCB1',\n",
    "        'SMARCD1', 'SMARCD2', 'SMARCD3', 'SMARCA2', 'SMARCA4', 'BCL7A',\n",
    "        'BCL7B', 'BCL7C', 'ACTB', 'ACTL6A')\n",
    "\n",
    "# assume top_15_prots is your DataFrame\n",
    "top_15_prots['is_pbaf'] = top_15_prots['protein'].isin(PBAF)\n",
    "\n",
    "# define colours for True/False\n",
    "palette = {True: \"steelblue\",   # e.g. red for PBAF\n",
    "           False: \"darkgrey\"}  # grey for non-PBAF\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(\n",
    "    data=top_15_prots,\n",
    "    x=\"protein\", y=\"diff\",\n",
    "    hue=\"is_pbaf\",\n",
    "    dodge=False,            # so bars are not side-by-side\n",
    "    palette=palette,\n",
    "    edgecolor='black'\n",
    ")\n",
    "pbaf_patch   = mpatches.Patch(color='steelblue', label='PBAF proteins')\n",
    "other_patch  = mpatches.Patch(color='darkgrey', label='Other proteins')\n",
    "plt.legend(handles=[pbaf_patch, other_patch],\n",
    "           title=\"\", loc='lower right', frameon=False)\n",
    "plt.xlabel(\"Protein\")\n",
    "plt.ylabel(\"Abundance Change\")\n",
    "plt.title(\"Top 15 Downregulated Proteins After Simulated PBRM1 KO\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
